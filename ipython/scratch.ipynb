{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexbraun/google_drive/code/projects/sparse/python\n",
      "/Users/alexbraun/google_drive/code/projects/texture_classifier/python\n"
     ]
    }
   ],
   "source": [
    "%cd ~/google_drive/code/projects/sparse/python\n",
    "from sparse.core.sparse_dataframe import SparseDataFrame\n",
    "from sparse.utilities.utils import *\n",
    "\n",
    "%cd ~/google_drive/code/projects/texture_classifier/python\n",
    "# %cd /home/ubuntu/texture_classifier/python\n",
    "import multiprocessing\n",
    "import PIL\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from pandas.io.pytables import HDFStore\n",
    "# from plotly import plotly\n",
    "# from plotly.graph_objs import *\n",
    "\n",
    "import core.utils\n",
    "reload(core.utils)\n",
    "from core.utils import *\n",
    "\n",
    "import core.image_scanner\n",
    "reload(core.image_scanner)\n",
    "from core.image_scanner import ImageScanner\n",
    "\n",
    "import core.pipeline\n",
    "reload(core.pipeline)\n",
    "from core.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = os.listdir('/Users/alexbraun/Downloads/data/arroway_wood/data')\n",
    "info = filter(lambda x: re.search('diffuse', x), info)\n",
    "info = filter(lambda x: re.search('_a_', x), info)\n",
    "info = map(lambda x: re.search('.*_(.*)_diffuse', x).group(1), info)\n",
    "info = sorted(info)\n",
    "idwood = pd.read_csv('/Users/alexbraun/google_drive/code/projects/texture_classifier/data/identifying_wood_data.csv')\n",
    "\n",
    "temp = list(set(list(chain(*map(lambda x: x.split('-'), info)))))\n",
    "def has_keyword(item):\n",
    "    output = []\n",
    "    for regex in temp:\n",
    "        found = re.search(regex, item, re.IGNORECASE)\n",
    "        if found:\n",
    "            output.append(found.group(0))\n",
    "    if output:\n",
    "        return ' '.join(output)\n",
    "    return False\n",
    "    \n",
    "for col in ['common_name']:#, 'scientific_name']:\n",
    "    new_col = 'found_' + col\n",
    "    idwood[new_col] = None\n",
    "    idwood[new_col] = idwood[col].apply(has_keyword)\n",
    "    \n",
    "x = idwood[(idwood.found_common_name != False)]# | (idwood.found_scientific_name != False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sci = x[x.found_scientific_name != False].index\n",
    "# x.found_scientific_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = x.found_common_name.apply(lambda x: x not in ['fir', 'cedar', 'white', 'black', 'red'])\n",
    "x = x[mask][['common_name', 'found_common_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexbraun/google_drive/code/projects/texture_classifier/python\n",
      "/Users/alexbraun/Documents/data/texture_classifier/data/msgpack\n"
     ]
    }
   ],
   "source": [
    "# %%snakeviz\n",
    "%cd /Users/alexbraun/google_drive/code/projects/texture_classifier/python \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, ShuffleSplit\n",
    "import pickle\n",
    "import core.image_scanner\n",
    "reload(core.image_scanner)\n",
    "from core.image_scanner import ImageScanner\n",
    "\n",
    "import core.utils\n",
    "reload(core.utils)\n",
    "from core.utils import *\n",
    "\n",
    "# root = '/Volumes/abraun.backup.01/project folders/texture_classifier/data'\n",
    "root = '/Users/alexbraun/Documents/data/texture_classifier/data/msgpack'\n",
    "%cd /Users/alexbraun/Documents/data/texture_classifier/data/msgpack\n",
    "\n",
    "# spec = ['texture', 'image_id', 'image_class', 'common_name', 'pass_', 'extension']\n",
    "# info = get_series_info(root, spec)\n",
    "info = pd.read_pickle('/Users/alexbraun/Documents/data/texture_classifier/data/info.pkl')\n",
    "info = info[(info.image_class == 'a') & (info.pass_ == 'diffuse')]\n",
    "# info = info.head()\n",
    "\n",
    "patches = 1000\n",
    "\n",
    "y = Series(info.index).apply(lambda x: [x]) * patches\n",
    "y = list(chain(*y.tolist()))\n",
    "y = Series(y)\n",
    "\n",
    "params = {\n",
    "            'aspect_ratio':     1,\n",
    "            'min_size':         0.05,\n",
    "            'max_size':         0.1,\n",
    "            'patches':          patches,\n",
    "            'patch_resolution': (100, 100),\n",
    "            'rotation':         'random'\n",
    "}\n",
    "\n",
    "data = []\n",
    "for file_ in info.fullpath:\n",
    "    data.append(pd.read_msgpack(file_))\n",
    "    \n",
    "data = pd.concat(data, axis=0)\n",
    "data['y'] = y\n",
    "\n",
    "# index = data.index\n",
    "# np.random.shuffle(index.tolist())\n",
    "# data = data.ix[index]\n",
    "\n",
    "# data = data.head(10000)\n",
    "\n",
    "# X = data.drop('y', axis=1)\n",
    "# y = data['y']\n",
    "\n",
    "# index = data.index.tolist()\n",
    "# np.random.shuffle(index)\n",
    "# data.index = index\n",
    "# X = np.array(data.ix[index])\n",
    "\n",
    "# data.to_msgpack('../random_scan.100.100x100.05-10.random.msgpack')\n",
    "# shuffle data\n",
    "# index = data.index.tolist()\n",
    "# np.random.shuffle(index)\n",
    "# data = data.ix[index]\n",
    "\n",
    "# with open('../random_scan.1000.100x100.05-10.random.pkl', 'w') as pkl:\n",
    "#     pickle.dump(data, pkl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "params = {\n",
    "    'n_estimators':        [10],\n",
    "#     'criterion':'          gini',\n",
    "#     'max_depth':           None,\n",
    "#     'min_samples_split':   2,\n",
    "#     'min_samples_leaf':    1,\n",
    "#     'max_features':'       'auto',\n",
    "#     'max_leaf_nodes':      None,\n",
    "#     'bootstrap':           True,\n",
    "#     'oob_score':           False,\n",
    "    'n_jobs':              [-1]\n",
    "#     'random_state':        None,\n",
    "#     'verbose':             0,\n",
    "#     'min_density':         None,\n",
    "#     'compute_importances': None\n",
    "}\n",
    "# train, test = [x for x in StratifiedShuffleSplit(y, n_iter=1, test_size=0.2)][0]\n",
    "# train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "grid = GridSearchCV(clf, params)\n",
    "# %time grid.fit(X.ix[test], y.ix[test])\n",
    "# print(grid.best_score_)\n",
    "# print(grid.best_estimator_.score(X.ix[test], y.ix[test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 24), (24, 48), (48, 72), (72, 96), (96, 100)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch(info, hdf_path):\n",
    "    info.reset_index(drop=True, inplace=True)\n",
    "    n = info.shape[0]\n",
    "    indices = range(0, n, processes)\n",
    "    indices.append(n)\n",
    "    indices = zip(indices, indices[1:])\n",
    "    \n",
    "    batch_path = os.path.join(hdf_path, '.batch')\n",
    "    os.mkdir(hdf_path)\n",
    "    \n",
    "    for i, (start, stop) in enumerate(indices):       \n",
    "        pool = multiprocessing.Pool(processes=processes)\n",
    "        \n",
    "        batch = info.ix[range(star, stop + 1)]\n",
    "        iterable = [(row.to_frame().T, features) for i, row in batch.iterrows()]\n",
    "        data = pool.map(_multi_get_data, iterable)\n",
    "        pool.close()\n",
    "        \n",
    "        data = pd.concat(data, axis=0)\n",
    "        filename = 'data.' + str(i).zfill(4) + '.hdf.batch'\n",
    "        hdf = HDFStore(os.path.join(batch_path, filename))\n",
    "        hdf['data'] = data\n",
    "        hdf.close()\n",
    "        \n",
    "        print('indices {:<10} - {:<10} written to {:<}'.format(start, stop, filename))\n",
    "    \n",
    "    batch = filter(lambda x: '.hdf.batch' in x, os.listdir(batch_path))\n",
    "    batch = map(os.path.abspath, batch)\n",
    "    data = [pd.read_hdf(x, 'data') for x in batch]\n",
    "    data = pd.concat(data, axis=0, ignore_index=True)\n",
    "    \n",
    "    # shuffle data to destroy serial correlations\n",
    "    index = data.index.tolist()\n",
    "    np.random.shuffle(index)\n",
    "    data = data.ix[index]\n",
    "    data.reset_index(drop=True, inplace=True)   \n",
    "    \n",
    "    hdf = HDFStore(os.join(hdf_path, 'data.hdf')\n",
    "    hdf['data'] = data\n",
    "    hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
